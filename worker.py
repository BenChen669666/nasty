"""This module takes jobs from the generator.

It will work them off in parallel."""
import json
from multiprocessing import Pool
from pathlib import Path
from typing import Dict

from tweet import Tweet


# The default worker. Takes the jobs, generated by generator.py, to download and
# save them
def work(job: Dict) -> None:
    """Takes a job from jobs.jsonl and executes it."""
    keyword = job["keyword"]
    since = job["start_date"]
    until = job["end_date"]
    lang = job["lang"]
    Tweet.extract(f"https://mobile.twitter.com/search?q={keyword}"
                  f"%20since%3A{since}"
                  f"%20until%3A{until}"
                  f"%20lang%3A{lang}&src=typed_query",
                  f"out/{keyword}-{since}.jsonl.gz",
                  job)


def try_failed(job: Dict) -> None:
    """Takes a failed job, from failedJobs.jsonl, and executes it."""
    keyword = job["keyword"]
    since = job["start_date"]
    until = job["end_date"]
    lang = job["lang"]
    Tweet.extract(f"https://mobile.twitter.com/search?q={keyword}"
                  f"%20since%3A{since}"
                  f"%20until%3A{until}"
                  f"%20lang%3A{lang}&src=typed_query",
                  f"out/{keyword}-{since}.jsonl.gz",
                  job)


def test_failed_jobs() -> None:
    """Check's if a job is not yet done and was not collected to failedJobs.
    Corrects failedJobs."""
    with open("jobs.jsonl") as jobs_file:
        for job in jobs_file:
            job = json.loads(job)
            keyword = job["keyword"]
            since = job["start_date"]
            path = Path(f"out/{keyword}-{since}.jsonl.gz")
            if not path.is_file():
                with open("failedJobs.jsonl") as failed_jobs_file:
                    for fail in failed_jobs_file:
                        fail = json.loads(fail)
                        if fail == job:
                            break
                    else:
                        with open("failedJobs.jsonl", "a") as correct_fails:
                            correct_fails.write(json.dumps(job))
                            correct_fails.write("\n")


def run(process_count: int = 1, work_mode: bool = True) -> None:
    """
    This methods is used to run the worker from outer scope.

    :param process_count: (Optional) Number of parallel processes (Default: 1)
    :param work_mode: (Optional) If true just run through your jobs,
        if false look up your missing and failed jobs. (Default: True)
    :return:
    """
    with Pool(processes=process_count) as pool:
        if work_mode:
            with open("jobs.jsonl") as to_do_file:
                tasks = []
                for todo in to_do_file:
                    tasks.append(json.loads(todo))
            pool.map(work, tasks)
        else:
            test_failed_jobs()
            with open("failedJobs.jsonl") as failed_file:
                tasks = []
                for failed in failed_file:
                    tasks.append(json.loads(failed))
            pool.map(try_failed, tasks)


if __name__ == '__main__':
    run(4, True)
